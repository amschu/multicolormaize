{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WELCOME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for using this software! Below is a tutorial to work your through the pipeline. \n",
    "\n",
    "Thankfully it's super straight forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "### **1) Setting up a conda enviroment**\n",
    "A conda enviroment is essentially a seperate and safe bubble to work on your code without risking changing additional settings you used for other projects. \n",
    "\n",
    "In this repository you can find an `envrioment.yml` file that can be used to help set up your enviroment to be the same as how this repository was ran on when developed.\n",
    "\n",
    "The [attached link](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) has additional resources related to setting up a conda enviroment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where do you find the `envrioment.yml` file?\n",
    "\n",
    "This is located in the `multicolormaize/` directory. \n",
    "\n",
    "Additionally, this is the path directory from where this `usage_examples.ipnb` is located: `../multicolormaize/enviroment.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code on how to create a new enviroment using `.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conda env create -n you_envirment_name -f environment.yml` --> create the enviroment using the enviroment file\n",
    "\n",
    "`conda activate myenv`  --> activate your enviroment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Files Break Down\n",
    "\n",
    "The pupose of this pipeline is to compare model predictions for pheno typic traits of interest in maize *(silk color, anther color, and specific leaf weight)*. Here are 3 types of data that can be used as input datasets \n",
    "\n",
    "#### 1) Data types examples\n",
    "- phenotypic traits\n",
    "- field drone imagery\n",
    "- phenolic compound accumulation in leaf tissue\n",
    "\n",
    "\n",
    "The following files are python files that will be used throughou the pipeline\n",
    "#### 2) Python scripts\n",
    "- datapreprocessing.py\n",
    "- runmodels.py\n",
    "- featureselection.py\n",
    "- multicolormaize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pipeline Breakdown\n",
    "Please note that the order of the files is **NOT** how you use the pipeline, but is how the pipeline works as a whole from start to end. The last step is how you actually implement the software decribed to that point.\n",
    "\n",
    "1) `datapreprocessing.py`\n",
    "This file would be the first step in the pipeline. Here you input a file location for your data of interst. In our example we will use RGB drone imagery data.\n",
    "**What does this script do:**\n",
    "- it removes duplicates\n",
    "- removes outliers\n",
    "- scales the data\n",
    "- addresses NAs in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) `runmodels.py`\n",
    "Here you input a cleaned data file outputted from step 1.\n",
    "**What does this script do:**\n",
    "- Undergoes the first splitting of the data into training and testing dataset\n",
    "- Takes the training dataset and does an additional splitting into a sub-training set and a validation set. A validation dataset allows us to evaluate the model without compromises our testing set. Using the validation model performances we can then identify the beset prediction model followed by top feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) `featureselections.py`\n",
    "After you generate model performances based on the validation sets, you will need to select the top perfoming model algorithm. This script takes the top performing.\n",
    "- qauntifying the most impactful features in predictions\n",
    "- generates new subset of original input data but only containing the top features. *(this helps cut down computational time)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) `multicolormaize.py`\n",
    "\n",
    "This is the script that actually runs all the above automatically.\n",
    "\n",
    "*NOTE!!!* In this script this is where you will need to change the path to the input file. Currently the script uses the provided test set in this `./tutorial` folder/directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************************************************************\n",
      "\n",
      "----------------------- \n",
      "STARTING TO LOAD DATA! \n",
      "-----------------------\n",
      "\n",
      "Yay, the file exists!\n",
      "\n",
      "---------------------------- \n",
      "DATA SUCCESSFULLY LOADED IN! \n",
      "----------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------ \n",
      "STARTING DATA CLEANING! \n",
      "-------------------------\n",
      "\n",
      "--- Removing duplicate rows ---\n",
      "\n",
      "Shape of df before dups rows meaned: (761, 836)\n",
      "\n",
      "Shape of df after dups rows meaned: (761, 836)\n",
      "\n",
      "--- Dropping unwanted columns ---\n",
      "\n",
      "--- Accounting for custom binary classes ---\n",
      "\n",
      "--- Converted Y_col to binary classes---\n",
      " 0    474\n",
      "1    287\n",
      "Name: AntherColor, dtype: int64\n",
      "\n",
      "--- Dropping/imputing columns with too many NAs ---\n",
      "\n",
      "Total number of columns dropped: 0\n",
      "\n",
      "--- Starting Scaling Data... ---\n",
      "\n",
      "--- Snapshot of final imputed data ---\n",
      "            AntherColor  0603_NC_red_SUM  ...  0603_NC_red_MEDIAN  0603_NC_red_STDEV\n",
      "GRIN                                     ...                                       \n",
      "Ames10254            0        -0.307057  ...           -0.379966           0.339749\n",
      "Ames10256            0        -0.535059  ...            0.174500          -0.021408\n",
      "Ames12726            0        -0.455299  ...            0.174500           0.733877\n",
      "Ames12731            1         1.190280  ...            1.006198           0.291194\n",
      "Ames12732            0        -1.807241  ...           -0.934431          -1.214349\n",
      "\n",
      "[5 rows x 5 columns]\n",
      "\n",
      "Output file saved as: widiv_2021drone_SilkandAntherColor_predicting_AntherColor_preprocessed.txt\n",
      "\n",
      "----------------------- \n",
      "DATA CLEANING COMPLETE! \n",
      "-----------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "-------------------------------------------- \n",
      "SPLITTING DATA INTO TRAINING AND TEST SETS \n",
      "--------------------------------------------\n",
      "X_train shape: (608, 834)\n",
      "y_train shape: (608,)\n",
      "X_test shape: (153, 834)\n",
      "y_test shape: (153,)\n",
      "\n",
      "---------------------------- \n",
      "DATA SUCCESSFULLY SPLIT! \n",
      "----------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------------------------- \n",
      "STARING TO RUN MODELS AND GENERATE FEATURE IMPORANCE SCORES! \n",
      "-------------------------------------------------------------\n",
      "\n",
      "--- Generating results ---\n",
      "\n",
      "The following models will be ran on the dataset:\n",
      "\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Random Forest\n",
      "K-Nearest Neighbors\n",
      "Support Vector Machine\n",
      "Gaussian Naive Bayes\n",
      "Multilayer Perceptron\n",
      "Linear Discriminant Analysis\n",
      "\n",
      "--- Saving the file containing all model results ---\n",
      "\n",
      "The perfomance metrics tracked are:\n",
      " Accuracy Precision Recall F1-score Confusion Matrix\n",
      "\n",
      "--- Generating feature importance scores for each model ---\n",
      "\n",
      "----------------------------------------------------------------- \n",
      "COMPLETED MODEL GENERATION WITH RESULTS AND FEATURE IMPORANTANCE! \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Results df:\n",
      "                    Logistic Regression  ... Linear Discriminant Analysis\n",
      "Accuracy                      0.601307  ...                     0.568627\n",
      "Precision                     0.605413  ...                     0.590091\n",
      "Recall                        0.601307  ...                     0.568627\n",
      "F1-score                      0.603135  ...                     0.574698\n",
      "Confusion Matrix  [[63, 32], [29, 29]]  ...         [[55, 40], [26, 32]]\n",
      "\n",
      "[5 rows x 8 columns]\n",
      "\n",
      "Feat importance df:\n",
      "                                           Logistic Regression  ...                       Linear Discriminant Analysis\n",
      "Feature     Index(['0603_NC_red_SUM', '0603_NC_red_MEAN', ...  ...  Index(['0603_NC_red_SUM', '0603_NC_red_MEAN', ...\n",
      "Importance  [0.29848576131001375, 0.08983267559044382, 0.1...  ...  [40.714697622934466, 21.651363491829933, 12.72...\n",
      "\n",
      "[2 rows x 4 columns]\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------- \n",
      "PLOTTING TOP FEATURES FOR EACH MODEL \n",
      "-------------------------------------\n",
      "\n",
      "--- Generating and saving feature importance figures ---\n",
      "\n",
      "K-Nearest_Neighbors feature importances file not found.\n",
      "Support_Vector_Machine feature importances file not found.\n",
      "Gaussian_Naive_Bayes feature importances file not found.\n",
      "Multilayer_Perceptron feature importances file not found.\n",
      "\n",
      "---------------------------------------------- \n",
      "EACH MODEL's TOP FEATURES HAVE BEEN COMPLETED! \n",
      "----------------------------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "--------------------------------------- \n",
      "GETTING THE TOP PERFORMING MODELS NAME \n",
      "----------------------------------------\n",
      "The top model is: Decision Tree\n",
      "\n",
      "------------------ \n",
      "GOT THE TOP MODEL! \n",
      "-------------------\n",
      "\n",
      "Top performing model was:\n",
      " Decision Tree\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------ \n",
      "GETTING THE TOP FEATURES OF THE TOP MODEL! \n",
      "-------------------------------------------\n",
      "\n",
      "--- Loading in the file associated to the top models features for ---\n",
      "              Feature  Importance\n",
      "0   0706_NC_blue_MEAN    0.036250\n",
      "1   0615_MS_red_STDEV    0.035579\n",
      "2     0816_MS_nir_MIN    0.034711\n",
      "3  0603_MS_nir_MEDIAN    0.031515\n",
      "4     0606_NC_green_5    0.031004\n",
      "              Feature  Importance\n",
      "0  0603_MS_nir_MEDIAN    0.080458\n",
      "1  0930_NC_blue_STDEV    0.068544\n",
      "2   0802_MS_green_MIN    0.066537\n",
      "3   0615_MS_red_STDEV    0.064597\n",
      "4     0810_MS_green_5    0.062792\n",
      "\n",
      "----------------------- \n",
      "TOP FEATURES LOADED IN! \n",
      "------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------ \n",
      "GENERATING NEW INPUT FILE! \n",
      "-------------------------------------------\n",
      "\n",
      "Feature importance dataframe shape: (20, 2)\n",
      "\n",
      "Top 20 Features:\n",
      "\n",
      "0603_MS_nir_MEDIAN\n",
      "0930_NC_blue_STDEV\n",
      "0802_MS_green_MIN\n",
      "0615_MS_red_STDEV\n",
      "0810_MS_green_5\n",
      "0603_MS_red_5\n",
      "0810_NC_red_STDEV\n",
      "0826_NC_green_STDEV\n",
      "0706_NC_blue_MEAN\n",
      "0628_MS_red_MEAN\n",
      "0816_MS_nir_MIN\n",
      "0930_NC_red_5\n",
      "0615_MS_redge_MEAN\n",
      "0606_NC_green_5\n",
      "0714_MS_redge_STDEV\n",
      "0910_MS_nir_MIN\n",
      "0726_MS_blue_MEAN\n",
      "0615_NC_red_SUM\n",
      "0916_NC_green_MEDIAN\n",
      "0611_MS_nir_5\n",
      "\n",
      "Length of top features list is: 20\n",
      "\n",
      "Original Input dataframe features shape: (761, 834) \n",
      "New dataframe after feature selections shape: (761, 20)\n",
      "\n",
      "-------------------------- \n",
      "NEW INPUT FILE GENERATED! \n",
      "--------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "-------------------------------------------- \n",
      "SPLITTING DATA INTO TRAINING AND TEST SETS \n",
      "--------------------------------------------\n",
      "X_train shape: (608, 20)\n",
      "y_train shape: (608,)\n",
      "X_test shape: (153, 20)\n",
      "y_test shape: (153,)\n",
      "\n",
      "---------------------------- \n",
      "DATA SUCCESSFULLY SPLIT! \n",
      "----------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------------------------- \n",
      "RERUN MODEL USING ONLY TOP MODEL ON TOP FEATURE DATASET \n",
      "-------------------------------------------------------------\n",
      "\n",
      "--- Generating results using feature selected data ---\n",
      "\n",
      "--- Saving new model results after feature selection ---\n",
      "\n",
      "--- Re-Generating feature importance scores for top model ---\n",
      "\n",
      "--------------------------------------------------------------- \n",
      "NEW MODEL RESULTS AND TOP FEATURES MADE WITH NEW FEAT DATASET! \n",
      "---------------------------------------------------------------\n",
      "\n",
      "Results df:\n",
      "                          Decision Tree\n",
      "Accuracy                      0.568627\n",
      "Confusion Matrix  [[63, 32], [34, 24]]\n",
      "F1-score                       0.56709\n",
      "Precision                      0.56574\n",
      "Recall                        0.568627\n",
      "\n",
      "Feat importance df:\n",
      "                                                 Decision Tree\n",
      "Feature     Index(['0603_MS_nir_MEDIAN', '0930_NC_blue_STD...\n",
      "Importance  [0.07903409993323117, 0.057112144841270515, 0....\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------------------------ \n",
      "PLOTTING TOP FEATURES FOR TOP MODEL AFTER FEATURE SELECTION \n",
      "------------------------------------------------------------\n",
      "\n",
      "--- ReGenerating and saving feature importance figures ---\n",
      "\n",
      "-------------------------------------------- \n",
      "TOP MODEL USING TOP FEAUTURES FIGURES MADE! \n",
      "--------------------------------------------\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "----------------------------------------------------------- \n",
      "CONGRATULATIONS! YOU HAVE RAN THE PIPELINE SUCESSFULLY! :) \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amsch\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "!python ../multicolormaize/multicolormaize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONGRATS YOU RAN THE PIPELINE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional information\n",
    "\n",
    "#### Unit testing\n",
    "Unit testing is a way to check that you code is doing what you actually want it to do. For example $1 + 1$ does actually $=2$\n",
    "\n",
    "So, each `.py` file used in the pipeline above had a unit test associated with it seen with a prefix `test_` *filename*`.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************************************************************\n",
      "\n",
      "----------------------- \n",
      "STARTING TO LOAD DATA! \n",
      "-----------------------\n",
      "\n",
      "Yay, the file exists!\n",
      "\n",
      "---------------------------- \n",
      "DATA SUCCESSFULLY LOADED IN! \n",
      "----------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------ \n",
      "STARTING DATA CLEANING! \n",
      "-------------------------\n",
      "\n",
      "--- Removing duplicate rows ---\n",
      "\n",
      "Shape of df before dups rows meaned: (761, 836)\n",
      "\n",
      "Shape of df after dups rows meaned: (761, 836)\n",
      "\n",
      "--- Dropping unwanted columns ---\n",
      "\n",
      "--- Accounting for custom binary classes ---\n",
      "\n",
      "--- Converted Y_col to binary classes---\n",
      " 0    474\n",
      "1    287\n",
      "Name: AntherColor, dtype: int64\n",
      "\n",
      "--- Dropping/imputing columns with too many NAs ---\n",
      "\n",
      "Total number of columns dropped: 0\n",
      "\n",
      "--- Starting Scaling Data... ---\n",
      "\n",
      "--- Snapshot of final imputed data ---\n",
      "            AntherColor  0603_NC_red_SUM  ...  0603_NC_red_MEDIAN  0603_NC_red_STDEV\n",
      "GRIN                                     ...                                       \n",
      "Ames10254            0        -0.307057  ...           -0.379966           0.339749\n",
      "Ames10256            0        -0.535059  ...            0.174500          -0.021408\n",
      "Ames12726            0        -0.455299  ...            0.174500           0.733877\n",
      "Ames12731            1         1.190280  ...            1.006198           0.291194\n",
      "Ames12732            0        -1.807241  ...           -0.934431          -1.214349\n",
      "\n",
      "[5 rows x 5 columns]\n",
      "\n",
      "Output file saved as: widiv_2021drone_SilkandAntherColor_predicting_AntherColor_preprocessed.txt\n",
      "\n",
      "----------------------- \n",
      "DATA CLEANING COMPLETE! \n",
      "-----------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "-------------------------------------------- \n",
      "SPLITTING DATA INTO TRAINING AND TEST SETS \n",
      "--------------------------------------------\n",
      "X_train shape: (608, 834)\n",
      "y_train shape: (608,)\n",
      "X_test shape: (153, 834)\n",
      "y_test shape: (153,)\n",
      "\n",
      "---------------------------- \n",
      "DATA SUCCESSFULLY SPLIT! \n",
      "----------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------------------------- \n",
      "STARING TO RUN MODELS AND GENERATE FEATURE IMPORANCE SCORES! \n",
      "-------------------------------------------------------------\n",
      "\n",
      "--- Generating results ---\n",
      "\n",
      "The following models will be ran on the dataset:\n",
      "\n",
      "Logistic Regression\n",
      "Decision Tree\n",
      "Random Forest\n",
      "K-Nearest Neighbors\n",
      "Support Vector Machine\n",
      "Gaussian Naive Bayes\n",
      "Multilayer Perceptron\n",
      "Linear Discriminant Analysis\n",
      "\n",
      "--- Saving the file containing all model results ---\n",
      "\n",
      "The perfomance metrics tracked are:\n",
      " Accuracy Precision Recall F1-score Confusion Matrix\n",
      "\n",
      "--- Generating feature importance scores for each model ---\n",
      "\n",
      "----------------------------------------------------------------- \n",
      "COMPLETED MODEL GENERATION WITH RESULTS AND FEATURE IMPORANTANCE! \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Results df:\n",
      "                    Logistic Regression  ... Linear Discriminant Analysis\n",
      "Accuracy                      0.601307  ...                     0.568627\n",
      "Precision                     0.605413  ...                     0.590091\n",
      "Recall                        0.601307  ...                     0.568627\n",
      "F1-score                      0.603135  ...                     0.574698\n",
      "Confusion Matrix  [[63, 32], [29, 29]]  ...         [[55, 40], [26, 32]]\n",
      "\n",
      "[5 rows x 8 columns]\n",
      "\n",
      "Feat importance df:\n",
      "                                           Logistic Regression  ...                       Linear Discriminant Analysis\n",
      "Feature     Index(['0603_NC_red_SUM', '0603_NC_red_MEAN', ...  ...  Index(['0603_NC_red_SUM', '0603_NC_red_MEAN', ...\n",
      "Importance  [0.29848576131001375, 0.08983267559044382, 0.1...  ...  [40.714697622934466, 21.651363491829933, 12.72...\n",
      "\n",
      "[2 rows x 4 columns]\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------- \n",
      "PLOTTING TOP FEATURES FOR EACH MODEL \n",
      "-------------------------------------\n",
      "\n",
      "--- Generating and saving feature importance figures ---\n",
      "\n",
      "K-Nearest_Neighbors feature importances file not found.\n",
      "Support_Vector_Machine feature importances file not found.\n",
      "Gaussian_Naive_Bayes feature importances file not found.\n",
      "Multilayer_Perceptron feature importances file not found.\n",
      "\n",
      "---------------------------------------------- \n",
      "EACH MODEL's TOP FEATURES HAVE BEEN COMPLETED! \n",
      "----------------------------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "--------------------------------------- \n",
      "GETTING THE TOP PERFORMING MODELS NAME \n",
      "----------------------------------------\n",
      "The top model is: Logistic Regression\n",
      "\n",
      "------------------ \n",
      "GOT THE TOP MODEL! \n",
      "-------------------\n",
      "\n",
      "Top performing model was:\n",
      " Logistic Regression\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------ \n",
      "GETTING THE TOP FEATURES OF THE TOP MODEL! \n",
      "-------------------------------------------\n",
      "\n",
      "--- Loading in the file associated to the top models features for ---\n",
      "           Feature  Importance\n",
      "0   0802_NC_blue_5    1.515674\n",
      "1  0816_MS_red_MIN    1.292104\n",
      "2   0802_MS_blue_5    1.182032\n",
      "3    0606_MS_nir_5    1.176892\n",
      "4  0603_MS_green_5    1.163901\n",
      "               Feature  Importance\n",
      "0      0816_MS_red_MIN    0.589191\n",
      "1       0628_NC_blue_5    0.511959\n",
      "2  0714_MS_redge_STDEV    0.494475\n",
      "3        0628_NC_red_5    0.487241\n",
      "4      0816_NC_red_MIN    0.465894\n",
      "\n",
      "----------------------- \n",
      "TOP FEATURES LOADED IN! \n",
      "------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------ \n",
      "GENERATING NEW INPUT FILE! \n",
      "-------------------------------------------\n",
      "\n",
      "Feature importance dataframe shape: (20, 2)\n",
      "\n",
      "Top 20 Features:\n",
      "\n",
      "0816_MS_red_MIN\n",
      "0628_NC_blue_5\n",
      "0714_MS_redge_STDEV\n",
      "0628_NC_red_5\n",
      "0816_NC_red_MIN\n",
      "0802_NC_red_STDEV\n",
      "0816_NC_blue_MIN\n",
      "0802_MS_redge_MIN\n",
      "0810_MS_blue_MIN\n",
      "0706_MS_red_STDEV\n",
      "0802_NC_blue_5\n",
      "0628_MS_nir_5\n",
      "0606_MS_nir_5\n",
      "0726_MS_nir_MEDIAN\n",
      "0603_MS_green_5\n",
      "0916_NC_green_MEDIAN\n",
      "0826_MS_nir_5\n",
      "0802_MS_blue_5\n",
      "0706_MS_blue_STDEV\n",
      "0830_MS_red_5\n",
      "\n",
      "Length of top features list is: 20\n",
      "\n",
      "Original Input dataframe features shape: (761, 834) \n",
      "New dataframe after feature selections shape: (761, 20)\n",
      "\n",
      "-------------------------- \n",
      "NEW INPUT FILE GENERATED! \n",
      "--------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "-------------------------------------------- \n",
      "SPLITTING DATA INTO TRAINING AND TEST SETS \n",
      "--------------------------------------------\n",
      "X_train shape: (608, 20)\n",
      "y_train shape: (608,)\n",
      "X_test shape: (153, 20)\n",
      "y_test shape: (153,)\n",
      "\n",
      "---------------------------- \n",
      "DATA SUCCESSFULLY SPLIT! \n",
      "----------------------------\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------------------------- \n",
      "RERUN MODEL USING ONLY TOP MODEL ON TOP FEATURE DATASET \n",
      "-------------------------------------------------------------\n",
      "\n",
      "--- Generating results using feature selected data ---\n",
      "\n",
      "--- Saving new model results after feature selection ---\n",
      "\n",
      "--- Re-Generating feature importance scores for top model ---\n",
      "\n",
      "--------------------------------------------------------------- \n",
      "NEW MODEL RESULTS AND TOP FEATURES MADE WITH NEW FEAT DATASET! \n",
      "---------------------------------------------------------------\n",
      "\n",
      "Results df:\n",
      "                   Logistic Regression\n",
      "Accuracy                     0.562092\n",
      "Confusion Matrix  [[77, 18], [49, 9]]\n",
      "F1-score                     0.512951\n",
      "Precision                     0.50581\n",
      "Recall                       0.562092\n",
      "\n",
      "Feat importance df:\n",
      "                                           Logistic Regression\n",
      "Feature     Index(['0816_MS_red_MIN', '0628_NC_blue_5', '0...\n",
      "Importance  [0.5891912342247989, 0.5119585059453893, 0.494...\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "------------------------------------------------------------ \n",
      "PLOTTING TOP FEATURES FOR TOP MODEL AFTER FEATURE SELECTION \n",
      "------------------------------------------------------------\n",
      "\n",
      "--- ReGenerating and saving feature importance figures ---\n",
      "\n",
      "-------------------------------------------- \n",
      "TOP MODEL USING TOP FEAUTURES FIGURES MADE! \n",
      "--------------------------------------------\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "----------------------------------------------------------- \n",
      "CONGRATULATIONS! YOU HAVE RAN THE PIPELINE SUCESSFULLY! :) \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amsch\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# run a unittest on the multicolormaize.py file\n",
    "!python ../multicolormaize/test_multicolormaize.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
